{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neurohari/BDA_course_Aalto/blob/master/Copy_of_nlb_technical_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msN59Bp0ORU5"
      },
      "source": [
        "<center> <a href=\"https://githubtocolab.com/neurallatents/nlb_workshop/blob/main/nlb_technical/nlb_technical_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrqeOIPiORU8"
      },
      "source": [
        "# NLB 2022 Workshop: Technical Walkthrough\n",
        "\n",
        "## 1 Introduction\n",
        "\n",
        "### 1.1 Background\n",
        "\n",
        "Neural Latents Benchmark '21 (NLB'21) is a benchmark suite aimed at standardizing evaluation of latent variable models of neural spiking activity spanning a variety of tasks and brain areas. The primary objective of the challenge is to infer the firing rates of a set of held-out neurons given the spiking activity of held-in neurons, a procedure called co-smoothing.\n",
        "\n",
        "The benchmark suite features several datasets from experiments spanning a range of behaviors and brain regions, but they are all provided in the standard Neurodata Without Borders format and available on [DANDI](https://dandiarchive.org). The benchmark challenge itself is hosted on the platform [EvalAI](https://eval.ai), where model predictions can be submitted and automatically evaluated on private evaluation data.\n",
        "\n",
        "To facilitate participation in the competition, we provide the code package [`nlb_tools`](https://github.com/neurallatents/nlb_tools), which has functions for data preprocessing and submission preparation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9rwFKzmORU9"
      },
      "source": [
        "### 1.2 Notebook Overview\n",
        "This notebook will cover all of the steps in participating in NLB'21, including:\n",
        "1. **Dataset download** - getting dataset files from DANDI on to your machine\n",
        "2. **Data loading and preprocessing** - using `nlb_tools` to extract the data we expect you to model\n",
        "3. **Modeling neural data** - in this notebook, demonstrating one potential modeling approach using an RNN\n",
        "4. **Submitting and evaluating model predictions** - packaging predictions for submission to EvalAI or local evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKASkTBmORU_"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main.png?raw=true\" width=\"480\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s65Ur4yaORVA"
      },
      "source": [
        "### 1.2 Setup\n",
        "\n",
        "First, we need to install and import the packages we need for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lO45rRlcORVA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70fd7477-432c-4ee6-ee9b-2ab5b40521b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/neurallatents/nlb_tools.git\n",
            "  Cloning https://github.com/neurallatents/nlb_tools.git to /tmp/pip-req-build-2sasnqqc\n",
            "  Running command git clone -q https://github.com/neurallatents/nlb_tools.git /tmp/pip-req-build-2sasnqqc\n",
            "Collecting pandas<=1.3.4,>=1.0.0\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: h5py<4,>=2.9 in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (3.1.0)\n",
            "Collecting pynwb\n",
            "  Downloading pynwb-2.0.0-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py<4,>=2.9->nlb-tools==0.0.1) (1.5.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.4,>=1.0.0->nlb-tools==0.0.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.4,>=1.0.0->nlb-tools==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<=1.3.4,>=1.0.0->nlb-tools==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pynwb->nlb-tools==0.0.1) (57.4.0)\n",
            "Collecting hdmf<4,>=3.1.1\n",
            "  Downloading hdmf-3.2.1-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 41.0 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml<1,>=0.16\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema<5,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (5.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (4.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema<5,>=2.6.0->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (3.7.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 33.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nlb-tools==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nlb-tools==0.0.1) (3.1.0)\n",
            "Building wheels for collected packages: nlb-tools\n",
            "  Building wheel for nlb-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlb-tools: filename=nlb_tools-0.0.1-py3-none-any.whl size=32989 sha256=2a2a5a60a1ad6d912af6c289466ee932519807371a234d777d95242518da3cb5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zr_h3eix/wheels/2a/e3/dc/2d4b1ad404062cfe0a6b190a5ea438d97d3a82a023eb5f6e8b\n",
            "Successfully built nlb-tools\n",
            "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, pandas, hdmf, pynwb, nlb-tools\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed hdmf-3.2.1 nlb-tools-0.0.1 pandas-1.3.4 pynwb-2.0.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting dandi\n",
            "  Downloading dandi-0.36.0-py3-none-any.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting click-didyoumean\n",
            "  Downloading click_didyoumean-0.3.0-py3-none-any.whl (2.7 kB)\n",
            "Collecting keyring\n",
            "  Downloading keyring-23.5.0-py3-none-any.whl (33 kB)\n",
            "Collecting etelemetry>=0.2.2\n",
            "  Downloading etelemetry-0.3.0-py3-none-any.whl (6.3 kB)\n",
            "Collecting pyout!=0.6.0,>=0.5\n",
            "  Downloading pyout-0.7.2-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 99 kB/s \n",
            "\u001b[?25hCollecting keyrings.alt\n",
            "  Downloading keyrings.alt-4.1.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from dandi) (4.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from dandi) (21.3)\n",
            "Collecting interleave~=0.1\n",
            "  Downloading interleave-0.2.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pynwb!=1.1.0,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from dandi) (2.0.0)\n",
            "Collecting pycryptodomex\n",
            "  Downloading pycryptodomex-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from dandi) (1.4.4)\n",
            "Requirement already satisfied: ruamel.yaml<1,>=0.15 in /usr/local/lib/python3.7/dist-packages (from dandi) (0.17.21)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (from dandi) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from dandi) (7.1.2)\n",
            "Collecting fasteners\n",
            "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
            "Collecting pydantic>=1.9.0\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 41.2 MB/s \n",
            "\u001b[?25hCollecting fscacher\n",
            "  Downloading fscacher-0.2.0-py3-none-any.whl (11 kB)\n",
            "Collecting zarr~=2.10\n",
            "  Downloading zarr-2.11.0-py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests~=2.20 in /usr/local/lib/python3.7/dist-packages (from dandi) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dandi) (2.8.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from dandi) (1.1.0)\n",
            "Collecting semantic-version\n",
            "  Downloading semantic_version-2.9.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from dandi) (8.0.1)\n",
            "Collecting dandischema~=0.5.1\n",
            "  Downloading dandischema-0.5.2-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 405 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dandischema~=0.5.1->dandi) (3.10.0.2)\n",
            "Requirement already satisfied: jsonschema[format] in /usr/local/lib/python3.7/dist-packages (from dandischema~=0.5.1->dandi) (4.3.3)\n",
            "Collecting ci-info>=0.2\n",
            "  Downloading ci_info-0.2.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting email-validator>=1.0.3\n",
            "  Downloading email_validator-1.1.3-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from email-validator>=1.0.3->pydantic>=1.9.0->dandi) (2.10)\n",
            "Collecting dnspython>=1.15.0\n",
            "  Downloading dnspython-2.2.0-py3-none-any.whl (266 kB)\n",
            "\u001b[K     |████████████████████████████████| 266 kB 60.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (57.4.0)\n",
            "Requirement already satisfied: pandas<2,>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (1.3.4)\n",
            "Requirement already satisfied: h5py<4,>=2.9 in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (3.1.0)\n",
            "Requirement already satisfied: hdmf<4,>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (3.2.1)\n",
            "Requirement already satisfied: numpy<1.22,>=1.16 in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (1.21.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py<4,>=2.9->pynwb!=1.1.0,>=1.0.3->dandi) (1.5.2)\n",
            "Requirement already satisfied: scipy<2,>=1.1 in /usr/local/lib/python3.7/dist-packages (from hdmf<4,>=3.1.1->pynwb!=1.1.0,>=1.0.3->dandi) (1.4.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.5.1->dandi) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.5.1->dandi) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.5.1->dandi) (5.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema[format]->dandischema~=0.5.1->dandi) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.0.5->pynwb!=1.1.0,>=1.0.3->dandi) (2018.9)\n",
            "Collecting blessings\n",
            "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dandi) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->dandi) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->dandi) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->dandi) (3.0.4)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml<1,>=0.15->dandi) (0.2.6)\n",
            "Collecting asciitree\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "Collecting numcodecs>=0.6.4\n",
            "  Downloading numcodecs-0.9.1-cp37-cp37m-manylinux2010_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 39.3 MB/s \n",
            "\u001b[?25hCollecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.1.0-py3-none-any.whl (10 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting SecretStorage>=3.2\n",
            "  Downloading SecretStorage-3.3.1-py3-none-any.whl (15 kB)\n",
            "Collecting jeepney>=0.4.2\n",
            "  Downloading jeepney-0.7.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.0\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 24.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->dandi) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->dandi) (2.21)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->dandi) (3.0.7)\n",
            "Building wheels for collected packages: asciitree\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5050 sha256=bbd06d3818ce6555d10f9b252b408b7f7673f286ea00f4a55919a8d4b16da286\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/38/0def51e15add93bff3f4bf9c248b94db0839b980b8535e72a0\n",
            "Successfully built asciitree\n",
            "Installing collected packages: dnspython, arrow, webcolors, uri-template, rfc3987, rfc3339-validator, pydantic, jsonpointer, jeepney, isoduration, fqdn, email-validator, cryptography, SecretStorage, numcodecs, fasteners, ci-info, blessings, asciitree, zarr, semantic-version, pyout, pycryptodomex, keyrings.alt, keyring, interleave, fscacher, etelemetry, dandischema, click-didyoumean, dandi\n",
            "Successfully installed SecretStorage-3.3.1 arrow-1.2.2 asciitree-0.3.3 blessings-1.7 ci-info-0.2.0 click-didyoumean-0.3.0 cryptography-36.0.1 dandi-0.36.0 dandischema-0.5.2 dnspython-2.2.0 email-validator-1.1.3 etelemetry-0.3.0 fasteners-0.17.3 fqdn-1.5.1 fscacher-0.2.0 interleave-0.2.0 isoduration-20.11.0 jeepney-0.7.1 jsonpointer-2.2 keyring-23.5.0 keyrings.alt-4.1.0 numcodecs-0.9.1 pycryptodomex-3.14.1 pydantic-1.9.0 pyout-0.7.2 rfc3339-validator-0.1.4 rfc3987-1.3.8 semantic-version-2.9.0 uri-template-1.1.0 webcolors-1.11.1 zarr-2.11.0\n"
          ]
        }
      ],
      "source": [
        "# Uncomment the ones you need:\n",
        "\n",
        "# nlb_tools\n",
        "!pip install git+https://github.com/neurallatents/nlb_tools.git \n",
        "\n",
        "# PyTorch (for modeling)\n",
        "!pip install torch\n",
        "\n",
        "# DANDI CLI tool (optional, can use website instead)\n",
        "!pip install dandi\n",
        "\n",
        "# EvalAI-CLI (optional, can use website instead)\n",
        "# !pip install evalai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbrtLgCmORVD"
      },
      "source": [
        "## 2 Data Preparation\n",
        "\n",
        "### 2.1 Dataset Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuVxnQXrORVE"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_download.png?raw=true\" width=\"800\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkQl3tdwORVF"
      },
      "source": [
        "The datasets are available on the platform DANDI. They can be downloaded directly from the website or by using the DANDI CLI tool, as shown below. For this notebook, we will be using the MC_Maze_Large dataset, which is available from [here](https://dandiarchive.org/dandiset/000138). Links to the other datasets can be found on [our website](https://neurallatents.github.io/datasets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "niDZcsOOORVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7ed93d-d54f-4481-eea7-6c526ec9c675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-27 17:59:32,583 [    INFO] NumExpr defaulting to 2 threads.\n",
            "PATH                                                              SIZE      DONE            DONE% CHECKSUM STATUS          MESSAGE   \n",
            "dandiset.yaml                                                                                              done            updated   \n",
            "sub-Jenkins/sub-Jenkins_ses-large_desc-test_ecephys.nwb           802.4 kB  802.4 kB         100%    ok    done                      \n",
            "sub-Jenkins/sub-Jenkins_ses-large_desc-train_behavior+ecephys.nwb 148.6 MB  148.6 MB         100%    ok    done                      \n",
            "Summary:                                                          149.4 MB  149.4 MB                       3 done          1 updated \n",
            "                                                                            100.00%                                                  \n",
            "2022-02-27 17:59:45,629 [    INFO] Logs saved in /root/.cache/dandi-cli/log/20220227175930Z-211.log\n"
          ]
        }
      ],
      "source": [
        "!dandi download https://dandiarchive.org/dandiset/000138"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X87Qf4-qORVH"
      },
      "source": [
        "The above line will download two files into the folder `./000138/sub-Jenkins/`. Next, we'll get the path of the downloaded files and list them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EPx2cpdEORVH",
        "outputId": "0bcd00bf-1c58-4013-f7bd-61981f89bc28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sub-Jenkins_ses-large_desc-test_ecephys.nwb',\n",
              " 'sub-Jenkins_ses-large_desc-train_behavior+ecephys.nwb']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "curr_path = os.getcwd()\n",
        "fpath = curr_path + '/000138/sub-Jenkins/'\n",
        "os.listdir(fpath) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBNzkKW8ORVI"
      },
      "source": [
        "The file with 'desc-train' in its name is for training, while the file with 'desc-test' in its name is for final model evaluation. As we take a look at the data, we will see the differences between these two files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eAj-0EgORVJ"
      },
      "source": [
        "### 2.2 Dataset Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkhFtpYwORVJ"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_dataload.png?raw=true\" width=\"800\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uta8Ye7BORVJ"
      },
      "source": [
        "To get the NWB data into Python, we provide the `NWBDataset` class, which can load from the dataset files and perform simple preprocessing operations. To load a dataset, you instantiate an instance of NWBDataset and provide the path to the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IlywaWfiORVJ"
      },
      "outputs": [],
      "source": [
        "from nlb_tools.nwb_interface import NWBDataset\n",
        "\n",
        "dataset = NWBDataset(fpath=fpath) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjy4yq_7ORVL"
      },
      "source": [
        "### 2.3 Data Format\n",
        "\n",
        "The loaded data are primarily stored in two pandas DataFrames: `NWBDataset.data` and `NWBDataset.trial_info`.\n",
        "\n",
        "#### `NWBDataset.data`\n",
        "`NWBDataset.data` contains the continuous recorded data, like spike counts and kinematics. Each row consists of measurements taken at a particular timestep. Most importantly, spiking data from held-in units are labeled `spikes` and data from held-out units is labeled `heldout_spikes`.\n",
        "\n",
        "In the training data, all fields are available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sTvLKSsLORVL",
        "outputId": "768798db-82f3-4c23-848a-9da0fe02c559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8b83c8be-52d3-4241-bb0a-fc6b8d45015a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>signal_type</th>\n",
              "      <th colspan=\"2\" halign=\"left\">cursor_pos</th>\n",
              "      <th colspan=\"2\" halign=\"left\">eye_pos</th>\n",
              "      <th colspan=\"2\" halign=\"left\">hand_pos</th>\n",
              "      <th colspan=\"2\" halign=\"left\">hand_vel</th>\n",
              "      <th colspan=\"32\" halign=\"left\">heldout_spikes</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"40\" halign=\"left\">spikes</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>channel</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>1031</th>\n",
              "      <th>1051</th>\n",
              "      <th>1061</th>\n",
              "      <th>1071</th>\n",
              "      <th>1151</th>\n",
              "      <th>1172</th>\n",
              "      <th>1222</th>\n",
              "      <th>1271</th>\n",
              "      <th>1352</th>\n",
              "      <th>1372</th>\n",
              "      <th>1381</th>\n",
              "      <th>1472</th>\n",
              "      <th>1501</th>\n",
              "      <th>1571</th>\n",
              "      <th>1652</th>\n",
              "      <th>1662</th>\n",
              "      <th>1701</th>\n",
              "      <th>1722</th>\n",
              "      <th>1731</th>\n",
              "      <th>1841</th>\n",
              "      <th>2091</th>\n",
              "      <th>2161</th>\n",
              "      <th>2311</th>\n",
              "      <th>2351</th>\n",
              "      <th>2361</th>\n",
              "      <th>2381</th>\n",
              "      <th>2391</th>\n",
              "      <th>2422</th>\n",
              "      <th>2581</th>\n",
              "      <th>2622</th>\n",
              "      <th>2681</th>\n",
              "      <th>2701</th>\n",
              "      <th>...</th>\n",
              "      <th>2401</th>\n",
              "      <th>2411</th>\n",
              "      <th>2421</th>\n",
              "      <th>2431</th>\n",
              "      <th>2432</th>\n",
              "      <th>2451</th>\n",
              "      <th>2461</th>\n",
              "      <th>2471</th>\n",
              "      <th>2481</th>\n",
              "      <th>2491</th>\n",
              "      <th>2501</th>\n",
              "      <th>2521</th>\n",
              "      <th>2541</th>\n",
              "      <th>2561</th>\n",
              "      <th>2591</th>\n",
              "      <th>2611</th>\n",
              "      <th>2612</th>\n",
              "      <th>2621</th>\n",
              "      <th>2623</th>\n",
              "      <th>2631</th>\n",
              "      <th>2641</th>\n",
              "      <th>2651</th>\n",
              "      <th>2661</th>\n",
              "      <th>2671</th>\n",
              "      <th>2691</th>\n",
              "      <th>2692</th>\n",
              "      <th>2731</th>\n",
              "      <th>2732</th>\n",
              "      <th>2733</th>\n",
              "      <th>2734</th>\n",
              "      <th>2741</th>\n",
              "      <th>2743</th>\n",
              "      <th>2761</th>\n",
              "      <th>2771</th>\n",
              "      <th>2781</th>\n",
              "      <th>2791</th>\n",
              "      <th>2801</th>\n",
              "      <th>2881</th>\n",
              "      <th>2941</th>\n",
              "      <th>2951</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clock_time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40</th>\n",
              "      <td>-5.200000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.1</td>\n",
              "      <td>-5.195095</td>\n",
              "      <td>-31.606258</td>\n",
              "      <td>-1.481366</td>\n",
              "      <td>0.261386</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.001000</th>\n",
              "      <td>-5.199120</td>\n",
              "      <td>3.299442</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-5.196711</td>\n",
              "      <td>-31.605926</td>\n",
              "      <td>-1.727835</td>\n",
              "      <td>0.317635</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.002000</th>\n",
              "      <td>-5.198598</td>\n",
              "      <td>3.299110</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>-5.198551</td>\n",
              "      <td>-31.605623</td>\n",
              "      <td>-1.873343</td>\n",
              "      <td>0.342863</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.003000</th>\n",
              "      <td>-5.198598</td>\n",
              "      <td>3.299110</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-5.200457</td>\n",
              "      <td>-31.605240</td>\n",
              "      <td>-2.053902</td>\n",
              "      <td>0.334682</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.004000</th>\n",
              "      <td>-5.199120</td>\n",
              "      <td>3.299442</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>-5.202659</td>\n",
              "      <td>-31.604953</td>\n",
              "      <td>-2.238392</td>\n",
              "      <td>0.280908</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.095000</th>\n",
              "      <td>-5.200000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>1.6</td>\n",
              "      <td>8.8</td>\n",
              "      <td>-5.239548</td>\n",
              "      <td>-31.618091</td>\n",
              "      <td>-1.010222</td>\n",
              "      <td>-1.636778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.096000</th>\n",
              "      <td>-5.193734</td>\n",
              "      <td>3.308271</td>\n",
              "      <td>1.7</td>\n",
              "      <td>9.1</td>\n",
              "      <td>-5.240553</td>\n",
              "      <td>-31.619778</td>\n",
              "      <td>-0.964223</td>\n",
              "      <td>-1.878082</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.097000</th>\n",
              "      <td>-5.188833</td>\n",
              "      <td>3.311073</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.241476</td>\n",
              "      <td>-31.621848</td>\n",
              "      <td>-0.942786</td>\n",
              "      <td>-2.159794</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.098000</th>\n",
              "      <td>-5.187298</td>\n",
              "      <td>3.309505</td>\n",
              "      <td>0.3</td>\n",
              "      <td>9.7</td>\n",
              "      <td>-5.242439</td>\n",
              "      <td>-31.624097</td>\n",
              "      <td>-1.023439</td>\n",
              "      <td>-2.277696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:01:40.099000</th>\n",
              "      <td>-5.190729</td>\n",
              "      <td>3.305204</td>\n",
              "      <td>0.2</td>\n",
              "      <td>9.7</td>\n",
              "      <td>-5.243523</td>\n",
              "      <td>-31.626403</td>\n",
              "      <td>-0.917548</td>\n",
              "      <td>-2.321146</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 170 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b83c8be-52d3-4241-bb0a-fc6b8d45015a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b83c8be-52d3-4241-bb0a-fc6b8d45015a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b83c8be-52d3-4241-bb0a-fc6b8d45015a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "signal_type            cursor_pos           eye_pos        ... spikes               \n",
              "channel                         x         y       x     y  ...   2801 2881 2941 2951\n",
              "clock_time                                                 ...                      \n",
              "0 days 00:01:40         -5.200000  3.300000     0.6   1.1  ...    0.0  0.0  0.0  0.0\n",
              "0 days 00:01:40.001000  -5.199120  3.299442     2.5   0.6  ...    0.0  0.0  0.0  0.0\n",
              "0 days 00:01:40.002000  -5.198598  3.299110     2.5   0.4  ...    0.0  0.0  0.0  0.0\n",
              "0 days 00:01:40.003000  -5.198598  3.299110     2.7   0.5  ...    0.0  0.0  0.0  0.0\n",
              "0 days 00:01:40.004000  -5.199120  3.299442     2.8   0.8  ...    0.0  0.0  0.0  0.0\n",
              "...                           ...       ...     ...   ...  ...    ...  ...  ...  ...\n",
              "0 days 00:01:40.095000  -5.200000  3.300000     1.6   8.8  ...    0.0  0.0  0.0  0.0\n",
              "0 days 00:01:40.096000  -5.193734  3.308271     1.7   9.1  ...    0.0  0.0  0.0  0.0\n",
              "0 days 00:01:40.097000  -5.188833  3.311073     0.0  10.0  ...    0.0  0.0  0.0  0.0\n",
              "0 days 00:01:40.098000  -5.187298  3.309505     0.3   9.7  ...    0.0  1.0  0.0  0.0\n",
              "0 days 00:01:40.099000  -5.190729  3.305204     0.2   9.7  ...    0.0  0.0  0.0  0.0\n",
              "\n",
              "[100 rows x 170 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset.data.iloc[100000:100100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYeCITrbORVL"
      },
      "source": [
        "In the test data, only held-in spikes are available, while other data is concealed with NaNs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtOqwPSuORVM",
        "outputId": "731c7f3e-21f4-42ad-dac8-02f156bbd3a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>signal_type</th>\n",
              "      <th colspan=\"2\" halign=\"left\">cursor_pos</th>\n",
              "      <th colspan=\"2\" halign=\"left\">eye_pos</th>\n",
              "      <th colspan=\"2\" halign=\"left\">hand_pos</th>\n",
              "      <th colspan=\"2\" halign=\"left\">hand_vel</th>\n",
              "      <th colspan=\"2\" halign=\"left\">heldout_spikes</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"10\" halign=\"left\">spikes</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>channel</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>1031</th>\n",
              "      <th>1051</th>\n",
              "      <th>...</th>\n",
              "      <th>2741</th>\n",
              "      <th>2743</th>\n",
              "      <th>2761</th>\n",
              "      <th>2771</th>\n",
              "      <th>2781</th>\n",
              "      <th>2791</th>\n",
              "      <th>2801</th>\n",
              "      <th>2881</th>\n",
              "      <th>2941</th>\n",
              "      <th>2951</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clock_time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.001000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.002000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.003000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.004000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.095000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.096000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.097000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.098000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0 days 00:00:01.099000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 170 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "signal_type            cursor_pos     eye_pos     hand_pos     hand_vel      \\\n",
              "channel                         x   y       x   y        x   y        x   y   \n",
              "clock_time                                                                    \n",
              "0 days 00:00:01               NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "0 days 00:00:01.001000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "0 days 00:00:01.002000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "0 days 00:00:01.003000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "0 days 00:00:01.004000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "...                           ...  ..     ...  ..      ...  ..      ...  ..   \n",
              "0 days 00:00:01.095000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "0 days 00:00:01.096000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "0 days 00:00:01.097000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "0 days 00:00:01.098000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "0 days 00:00:01.099000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
              "\n",
              "signal_type            heldout_spikes       ... spikes                      \\\n",
              "channel                          1031 1051  ...   2741 2743 2761 2771 2781   \n",
              "clock_time                                  ...                              \n",
              "0 days 00:00:01                   NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "0 days 00:00:01.001000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "0 days 00:00:01.002000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "0 days 00:00:01.003000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "0 days 00:00:01.004000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "...                               ...  ...  ...    ...  ...  ...  ...  ...   \n",
              "0 days 00:00:01.095000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "0 days 00:00:01.096000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "0 days 00:00:01.097000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "0 days 00:00:01.098000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "0 days 00:00:01.099000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "signal_type                                      \n",
              "channel                2791 2801 2881 2941 2951  \n",
              "clock_time                                       \n",
              "0 days 00:00:01         0.0  0.0  0.0  0.0  0.0  \n",
              "0 days 00:00:01.001000  0.0  0.0  1.0  0.0  0.0  \n",
              "0 days 00:00:01.002000  0.0  0.0  0.0  0.0  0.0  \n",
              "0 days 00:00:01.003000  0.0  0.0  0.0  0.0  0.0  \n",
              "0 days 00:00:01.004000  0.0  0.0  0.0  0.0  0.0  \n",
              "...                     ...  ...  ...  ...  ...  \n",
              "0 days 00:00:01.095000  0.0  0.0  0.0  0.0  0.0  \n",
              "0 days 00:00:01.096000  0.0  0.0  0.0  0.0  0.0  \n",
              "0 days 00:00:01.097000  0.0  0.0  0.0  0.0  0.0  \n",
              "0 days 00:00:01.098000  0.0  0.0  0.0  0.0  0.0  \n",
              "0 days 00:00:01.099000  0.0  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[100 rows x 170 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.data.iloc[1000:1100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCeOVmmPORVM"
      },
      "source": [
        "\n",
        "#### `NWBDataset.trial_info`\n",
        "Each row of `trial_info` contains information about a particular experimental trial, such as when it begins and ends. As with the `NWBDataset.data`, almost all information is concealed in the test data. The field `split`, common to all of our provided datasets, indicates what data split a trial belongs to (explained in more detail in Section 2.4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "05Iwd2kxORVM",
        "outputId": "368e0bb0-e1c4-490a-927f-16050a66dd64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7e384250-0bd1-4c14-9d39-a2e092afb29e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trial_id</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>move_onset_time</th>\n",
              "      <th>split</th>\n",
              "      <th>trial_type</th>\n",
              "      <th>trial_version</th>\n",
              "      <th>maze_id</th>\n",
              "      <th>success</th>\n",
              "      <th>target_on_time</th>\n",
              "      <th>go_cue_time</th>\n",
              "      <th>rt</th>\n",
              "      <th>delay</th>\n",
              "      <th>num_targets</th>\n",
              "      <th>target_pos</th>\n",
              "      <th>num_barriers</th>\n",
              "      <th>barrier_pos</th>\n",
              "      <th>active_target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0 days 00:00:00</td>\n",
              "      <td>0 days 00:00:00.700000</td>\n",
              "      <td>0 days 00:00:00.250000</td>\n",
              "      <td>test</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0 days 00:00:00.800000</td>\n",
              "      <td>0 days 00:00:01.500000</td>\n",
              "      <td>0 days 00:00:01.050000</td>\n",
              "      <td>test</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0 days 00:00:01.600000</td>\n",
              "      <td>0 days 00:00:02.300000</td>\n",
              "      <td>0 days 00:00:01.850000</td>\n",
              "      <td>test</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0 days 00:00:02.400000</td>\n",
              "      <td>0 days 00:00:03.100000</td>\n",
              "      <td>0 days 00:00:02.650000</td>\n",
              "      <td>test</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0 days 00:00:03.200000</td>\n",
              "      <td>0 days 00:00:03.900000</td>\n",
              "      <td>0 days 00:00:03.450000</td>\n",
              "      <td>test</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>595</td>\n",
              "      <td>0 days 00:25:49.600000</td>\n",
              "      <td>0 days 00:25:52.636000</td>\n",
              "      <td>0 days 00:25:51.501000</td>\n",
              "      <td>train</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:25:50.405000</td>\n",
              "      <td>0 days 00:25:51.153000</td>\n",
              "      <td>348.0</td>\n",
              "      <td>748.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[[-105, 76]]</td>\n",
              "      <td>9.0</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>596</td>\n",
              "      <td>0 days 00:25:52.700000</td>\n",
              "      <td>0 days 00:25:55.746000</td>\n",
              "      <td>0 days 00:25:54.595000</td>\n",
              "      <td>train</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:25:53.467000</td>\n",
              "      <td>0 days 00:25:54.116000</td>\n",
              "      <td>479.0</td>\n",
              "      <td>649.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[[123, -81], [-130, -13], [123, 71]]</td>\n",
              "      <td>8.0</td>\n",
              "      <td>[[-65, -15, 14, 51], [-79, -55, 55, 6], [-103,...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>597</td>\n",
              "      <td>0 days 00:25:55.800000</td>\n",
              "      <td>0 days 00:25:58.801000</td>\n",
              "      <td>0 days 00:25:57.701000</td>\n",
              "      <td>val</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:25:56.545000</td>\n",
              "      <td>0 days 00:25:57.410000</td>\n",
              "      <td>291.0</td>\n",
              "      <td>865.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[[124, -79], [103, 83], [-105, 76]]</td>\n",
              "      <td>9.0</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>598</td>\n",
              "      <td>0 days 00:25:58.900000</td>\n",
              "      <td>0 days 00:26:01.956000</td>\n",
              "      <td>0 days 00:26:00.777000</td>\n",
              "      <td>train</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:25:59.613000</td>\n",
              "      <td>0 days 00:26:00.479000</td>\n",
              "      <td>298.0</td>\n",
              "      <td>866.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[[103, 83]]</td>\n",
              "      <td>9.0</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>599</td>\n",
              "      <td>0 days 00:26:02</td>\n",
              "      <td>0 days 00:26:05.021000</td>\n",
              "      <td>0 days 00:26:03.837000</td>\n",
              "      <td>train</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:26:02.877000</td>\n",
              "      <td>0 days 00:26:03.426000</td>\n",
              "      <td>411.0</td>\n",
              "      <td>549.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[[124, -79], [103, 83], [-105, 76]]</td>\n",
              "      <td>9.0</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>600 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e384250-0bd1-4c14-9d39-a2e092afb29e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7e384250-0bd1-4c14-9d39-a2e092afb29e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7e384250-0bd1-4c14-9d39-a2e092afb29e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     trial_id  ... active_target\n",
              "0           0  ...           NaN\n",
              "1           1  ...           NaN\n",
              "2           2  ...           NaN\n",
              "3           3  ...           NaN\n",
              "4           4  ...           NaN\n",
              "..        ...  ...           ...\n",
              "595       595  ...           0.0\n",
              "596       596  ...           2.0\n",
              "597       597  ...           1.0\n",
              "598       598  ...           0.0\n",
              "599       599  ...           0.0\n",
              "\n",
              "[600 rows x 18 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset.trial_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bThwPCq_ORVN"
      },
      "source": [
        "### 2.4 Data Splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwXaqkwnORVN"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/data_splits.png?raw=true\" width=\"480\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITXWQhVnORVN"
      },
      "source": [
        "The dataset is divided into train, val, and test splits. The train and val splits are contained within the training data file, while the test split is entirely in the test data file. This means that all data fields are available in the train and val splits, but only held-in data is available in the test split. The NLB'21 challenge has two phases based on these splits:\n",
        "1. In the Validation Phase, models will be evaluated on their val split predictions. This phase is offered for sanity checking results and building familiarity with the EvalAI platform.\n",
        "2. In the Test Phase, models will be evaluated on their test split predictions. This is the phase that is displayed on the public leaderboard. In this phase, the val split does not strictly need to be used for model validation, despite its name.\n",
        "\n",
        "In this notebook, we will prepare a submission for the Validation Phase, though the code can be easily modified for the Test Phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkQrvQlhORVN"
      },
      "source": [
        "### 2.5 Data Extraction\n",
        "\n",
        "#### 2.5.1 Resampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn5aw_VBORVN"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_resample.png?raw=true\" width=\"800\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3mBxwgWORVO"
      },
      "source": [
        "The raw data is at 1 ms resolution, but the NLB'21 challenge expects submissions to be at 5 ms resolution, so we will resample the data before doing any other processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m0Qw9lldORVO",
        "outputId": "1064802b-8472-4684-80f5-36faf5f07347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (1565021, 170)\n",
            "Bin width: 1 ms\n",
            "Resampled data shape: (313005, 170)\n",
            "Resampled bin width: 5 ms\n"
          ]
        }
      ],
      "source": [
        "print(f'Data shape: {dataset.data.shape}')\n",
        "print(f'Bin width: {dataset.bin_width} ms')\n",
        "dataset.resample(5)\n",
        "print(f'Resampled data shape: {dataset.data.shape}')\n",
        "print(f'Resampled bin width: {dataset.bin_width} ms')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUvkDulORVO"
      },
      "source": [
        "#### 2.5.2 Trial Alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGVFOZnnORVO"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_alignment.png?raw=true\" width=\"800\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrZFypudORVP"
      },
      "source": [
        "To simplify data preparation, we abstract other formatting steps into the functions `make_train_input_tensors` and `make_eval_input_tensors`. These wrapper functions format the raw data into tensors to be used for model training and evaluation. The primary processing performed by the functions is trial alignment.\n",
        "\n",
        "Trial alignment involves choosing a particular trial event, such as a go cue, and taking a fixed window of data around each occurrence of that event. For all of the datasets in NLB'21, we have chosen trial alignments based on the experimental design and past analyses on the data. For the MC_Maze_Large dataset, the selected trial alignment is 250 ms before to 450 ms after movement onset. Our wrapper functions will apply this alignment when given the correct dataset name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rg2EfQn4ORVP"
      },
      "outputs": [],
      "source": [
        "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOA50Cd8ORVP"
      },
      "source": [
        "`make_train_input_tensors` extracts the data available for model training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1CtVXDY9ORVP"
      },
      "outputs": [],
      "source": [
        "train_dict = make_train_input_tensors(dataset=dataset, \n",
        "                                      dataset_name='mc_maze_large', \n",
        "                                      trial_split='train', # trial_split=['train', 'val'], for Test phase\n",
        "                                      save_file=False, \n",
        "                                      include_behavior=True,\n",
        "                                      include_forward_pred=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PLtkRCiORVQ"
      },
      "source": [
        "`make_eval_input_tensors` extracts the data used to evaluate the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EXbEgzosORVQ"
      },
      "outputs": [],
      "source": [
        "eval_dict = make_eval_input_tensors(dataset=dataset,\n",
        "                                    dataset_name='mc_maze_large',\n",
        "                                    trial_split='val', # trial_split='test', for Test phase\n",
        "                                    save_file=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jb1HV1-ORVR"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_naming.png?raw=true\" width=\"600\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcN3YaN2ORVR"
      },
      "source": [
        "Both `make_train_input_tensors` and `make_eval_input_tensors` return dictionaries of tensors. \n",
        "\n",
        "The training dictionary contains:\n",
        "- train_spikes_heldin - spiking activity of held-in units on training trials \n",
        "- train_spikes_heldout - spiking activity of held-out units on training trials\n",
        "- train_spikes_heldin_forward - spiking activity of held-in units immediately after the trial period\n",
        "- train_spikes_heldout_forward - spiking activity of held-iout units immediately after the trial period\n",
        "\n",
        "The four different sets of data are visualized in the above figure. Each set of data is a 3D array with dimensions Trial x Time x Channel. \n",
        "\n",
        "<!---The tensor naming conventions are fairly straightforward. The tensors labeled 'heldin' contain spiking activity from held-in units. The tensors labeled 'heldout' contain spiking activity from held-out units. The tensors labeled 'forward' contain additional spiking activity occurring after each aligned trial window. All tensors have dimensions Batch x Time x Channel. --->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mn6sX6mCORVS",
        "outputId": "8c5d8bea-055d-4ae6-999e-88a7aa986453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['train_spikes_heldin', 'train_spikes_heldout', 'train_behavior', 'train_spikes_heldin_forward', 'train_spikes_heldout_forward'])\n"
          ]
        }
      ],
      "source": [
        "print(train_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tEbFf2TRORVS",
        "outputId": "6381cfe5-4749-496a-f3b9-f60c0cee1042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(375, 140, 122)\n",
            "(375, 140, 40)\n",
            "(375, 40, 122)\n",
            "(375, 40, 40)\n",
            "(375, 140, 2)\n"
          ]
        }
      ],
      "source": [
        "print(train_dict['train_spikes_heldin'].shape)\n",
        "print(train_dict['train_spikes_heldout'].shape)\n",
        "print(train_dict['train_spikes_heldin_forward'].shape)\n",
        "print(train_dict['train_spikes_heldout_forward'].shape)\n",
        "print(train_dict['train_behavior'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkSa_UKgORVT"
      },
      "source": [
        "The shapes above indicate that there are 375 training trials, 140 time bins during the trial, 40 time bins after the trial, 122 held-in units, and 40 held-out units in this dataset.\n",
        "\n",
        "Next, we look at the data used for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DcffjS-6ORVT",
        "outputId": "d1c0a64e-23d2-46db-f715-12f41d22e0f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['eval_spikes_heldin', 'eval_spikes_heldout'])\n"
          ]
        }
      ],
      "source": [
        "print(eval_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MT5r2T76ORVT",
        "outputId": "f2b84f81-54d9-4ac2-c562-6ffe6c547081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(125, 140, 122)\n"
          ]
        }
      ],
      "source": [
        "print(eval_dict['eval_spikes_heldin'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiZntm-FORVT"
      },
      "source": [
        "The output tensors of `make_eval_input_tensors` follow the same dimension ordering and naming conventions as those in `train_dict`. The only tensor that will always be returned in `eval_dict` is `'eval_spikes_heldin'`, as that is the only data available in the test split.\n",
        "\n",
        "If you are using a language other than Python for your model, you will want to save these tensors as HDF5 files by changing the `save_file=False` lines in the above examples to `save_file=True`. The HDF5 files will have the same key-value pairs as the dicts and can be loaded into other programs like [MATLAB](https://www.mathworks.com/help/matlab/import_export/importing-hierarchical-data-format-hdf5-files.html) or [R](https://www.bioconductor.org/packages/devel/bioc/vignettes/rhdf5/inst/doc/rhdf5.html) scripts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw5OCIiAORVT"
      },
      "source": [
        "## 3 Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpJzGDwZORVV"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_modeling.png?raw=true\" width=\"480\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuIN5_oAORVW"
      },
      "source": [
        "\n",
        "Now, we will apply a simple RNN to the challenge. Our code for training the model is available [here](https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/train.py), but we will load a pre-trained model instead of training here due to time constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64W6UaGkORVW"
      },
      "source": [
        "### 3.1 Model Definition\n",
        "\n",
        "We define a class that models the data with an RNN and uses an exponential mapping to firing rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSd9CqlVORVX"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "#import numpy as np\n",
        "#\n",
        "#class NLBRNN(torch.nn.Module):\n",
        "#    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3, device=None, dtype=None):\n",
        "#        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "#        super(NLBRNN, self).__init__()\n",
        "#        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
        "#        self.rnn = torch.nn.GRU(input_size=input_dim,\n",
        "#                                hidden_size=hidden_dim,\n",
        "#                                num_layers=num_layers,\n",
        "#                                batch_first=True,\n",
        "#                                dropout=(dropout if num_layers > 1 else 0.),\n",
        "#                                bidirectional=False,\n",
        "#                                **factory_kwargs)\n",
        "#        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
        "#        self.transform = torch.nn.Linear(hidden_dim, output_dim)\n",
        "#    \n",
        "#    def forward(self, X):\n",
        "#        output, hidden = self.rnn(self.dropout1(X))\n",
        "#        output = self.transform(self.dropout2(output))\n",
        "#        return torch.exp(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class hariRNN(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3,device=None, dtype=None):\n",
        "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "    super(hariRNN, self).__init__()\n",
        "    #self.dropout1 = torch.nn.Dropout(p=dropout)\n",
        "    self.rnn = torch.nn.GRU(input_size=input_dim,\n",
        "                                hidden_size=hidden_dim,\n",
        "                                num_layers=num_layers,\n",
        "                                batch_first=True,\n",
        "                                dropout=(dropout if num_layers > 1 else 0.),\n",
        "                                bidirectional=False,\n",
        "                                **factory_kwargs)\n",
        "    #self.dropout2 = torch.nn.Dropout(p=dropout)\n",
        "    self.transform_spike = torch.nn.Linear(hidden_dim, output_dim)\n",
        "    self.transform_behavior = torch.nn.Linear(hidden_dim, 2)\n",
        "\n",
        "  def forward(self, X):\n",
        "    output, hidden = self.rnn(X)\n",
        "    spiking_output = self.transform_spike(output)\n",
        "    behavior_output = self.transform_behavior(output)\n",
        "    return torch.exp(spiking_output), behavior_output"
      ],
      "metadata": {
        "id": "Qk4Y-FF7ZMqH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WU7iChOORVd"
      },
      "source": [
        "### 3.2 Input formatting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_jg5TirORVe"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/rnn_training_diagram.png?raw=true\" width=\"800\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx03M9MBORVe"
      },
      "source": [
        "We will next prepare our input and target data for training and evaluating the RNN. As seen in the figure above, we want the model to take held-in activity as input and predict firing rates for not only that held-in activity, but also held-out activity and future timesteps.\n",
        "\n",
        "Our input data will then only have held-in channels, and future timesteps will be filled with zeros to run the RNN forward with no inputs for forecasting. Our true output data will have held-in, held-out, and future activity to compute loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "7s5bdwLoORVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324775fe-e10f-4b12-e5ab-f4289636ac11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([375, 180, 122])\n",
            "torch.Size([375, 180, 162])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "training_input = torch.Tensor(\n",
        "    np.concatenate([\n",
        "        train_dict['train_spikes_heldin'], \n",
        "        np.zeros(train_dict['train_spikes_heldin_forward'].shape), # zeroed inputs for forecasting\n",
        "    ], axis=1))\n",
        "\n",
        "training_output = torch.Tensor(\n",
        "    np.concatenate([\n",
        "        np.concatenate([\n",
        "            train_dict['train_spikes_heldin'],\n",
        "            train_dict['train_spikes_heldin_forward'],\n",
        "        ], axis=1),\n",
        "        np.concatenate([\n",
        "            train_dict['train_spikes_heldout'],\n",
        "            train_dict['train_spikes_heldout_forward'],\n",
        "        ], axis=1),\n",
        "    ], axis=2))\n",
        "\n",
        "print(training_input.shape)\n",
        "print(training_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## HAri Code\n",
        "## combine spikes and behavior\n",
        "train_signal_heldin = np.concatenate([\n",
        "                                       train_dict['train_spikes_heldin'],\n",
        "                                       train_dict['train_behavior']\n",
        "\n",
        "                      ], axis=2)\n",
        "#dummy_vel_heldin_forward = 0.0*train_dict['train_spikes_heldin_forward']\n",
        "#dummy_vel_heldout_forward = 0.0*train_dict['train_spikes_heldout_forward']\n",
        "#dummy_vel_heldin_forward = dummy_vel_heldin_forward[:,:,:2]\n",
        "#dummy_vel_heldout_forward = dummy_vel_heldout_forward[:,:,:2]\n",
        "#train_signal_heldin_forward = np.concatenate([\n",
        "#                                       train_dict['train_spikes_heldin_forward'],\n",
        "#                                       dummy_vel_heldin_forward,\n",
        "#\n",
        "#                      ], axis=2)\n",
        "\n",
        "\n",
        "print(train_signal_heldin.shape)\n",
        "\n",
        "training_input = torch.Tensor(\n",
        "        train_signal_heldin)\n",
        "\n",
        "\n",
        "future_behavior = train_dict['train_behavior']\n",
        "future_spikes_heldin = train_dict['train_spikes_heldin']\n",
        "future_spikes_heldout = train_dict['train_spikes_heldout']\n",
        "\n",
        "future_behavior = future_behavior[1:,:,:]\n",
        "future_spikes_heldin = future_spikes_heldin[1:,:,:]\n",
        "future_spikes_heldout = future_spikes_heldout[1:,:,:]\n",
        "\n",
        "training_output = torch.Tensor(\n",
        "    np.concatenate([\n",
        "        future_behavior,\n",
        "        future_spikes_heldin,\n",
        "        future_spikes_heldout\n",
        "        ],axis=2\n",
        "        \n",
        "    )\n",
        ")\n",
        "\n",
        "print(training_input.shape)\n",
        "print(training_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-GBTNp1byUX",
        "outputId": "e8fe73ed-e66e-4ef9-cea2-ff754cca2a43"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(375, 140, 124)\n",
            "torch.Size([375, 140, 124])\n",
            "torch.Size([374, 140, 164])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vWsiBAjwvGl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vQGGkzjXvCGW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJkkp_NgORVf"
      },
      "source": [
        "We'll prepare the input for the final evaluation just like the training input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "MqGIsWNAORVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e177fe4e-506b-4d45-c0a2-3fc021280baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([125, 140, 124])\n"
          ]
        }
      ],
      "source": [
        "eval_input = torch.Tensor(\n",
        "              np.concatenate([\n",
        "        eval_dict['eval_spikes_heldin'],\n",
        "        np.zeros((eval_dict['eval_spikes_heldin'].shape[0], eval_dict['eval_spikes_heldin'].shape[1], 2))],axis=2))\n",
        "\n",
        "print(eval_input.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yPqJhobORVf"
      },
      "source": [
        "### 3.3 Model Training\n",
        "\n",
        "As mentioned above, the training script we used is available [here](https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/train.py). We essentially follow the diagram above but use an additional regularization scheme called coordinated dropout.\n",
        "\n",
        "Instead of training the model though, we'll load a pre-trained RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QqrcDv5ORVf"
      },
      "outputs": [],
      "source": [
        "# To download pre-trained model in Colab\n",
        "#!wget -O pretrained_rnn.ckpt https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/pretrained_rnn.ckpt?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Code for training the hariRNN\n",
        "class NLBRunner:\n",
        "    \"\"\"Class that handles training hariRNN\"\"\"\n",
        "    def __init__(self, model_init, model_cfg, data, train_cfg, use_gpu=False, num_gpus=1):\n",
        "        self.model = model_init(**model_cfg)\n",
        "        self.data = data\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            device = torch.device('cuda:0')\n",
        "            gpu_idxs = np.arange(min(num_gpus, torch.cuda.device_count())).tolist()\n",
        "            self.model = torch.nn.DataParallel(self.model.to(device), device_ids=gpu_idxs)\n",
        "            self.data = tuple([d.to(device) for d in self.data])\n",
        "        self.cd_ratio = train_cfg.get('cd_ratio', 0.2)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), \n",
        "                                          lr=train_cfg.get('lr', 1e-3), \n",
        "                                          weight_decay=train_cfg.get('alpha', 0.0))\n",
        "    \n",
        "    def make_cd_mask(self, train_input, train_output):\n",
        "        \"\"\"Creates boolean mask for coordinated dropout.\n",
        "        In coordinated dropout, a random set of inputs is zeroed out,\n",
        "        and only the corresponding outputs (i.e. same trial, timestep, and neuron)\n",
        "        are used to compute loss and update model weights. This prevents\n",
        "        exact spike times from being directly passed through the model.\n",
        "        \"\"\"\n",
        "        cd_ratio = self.cd_ratio\n",
        "        input_mask = torch.zeros((train_input.shape[0] * train_input.shape[1] * train_input.shape[2]), dtype=torch.bool)\n",
        "        idxs = torch.randperm(input_mask.shape[0])[:int(round(cd_ratio * input_mask.shape[0]))]\n",
        "        input_mask[idxs] = True\n",
        "        input_mask = input_mask.view((train_input.shape[0], train_input.shape[1], train_input.shape[2]))\n",
        "        output_mask = torch.ones(train_output.shape, dtype=torch.bool)\n",
        "        output_mask[:, :, :input_mask.shape[2]] = input_mask\n",
        "        return input_mask, output_mask\n",
        "    \n",
        "    def train_epoch(self):\n",
        "        \"\"\"Trains model for one epoch. \n",
        "        This simple script does not support splitting training samples into batches.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        # create mask for coordinated dropout\n",
        "        train_input, train_output, val_input, val_output, *_ = self.data\n",
        "        #input_mask, output_mask = self.make_cd_mask(train_input, train_output)\n",
        "        # mask inputs\n",
        "        #masked_train_input = train_input.clone()\n",
        "        #masked_train_input[input_mask] = 0.0\n",
        "        train_predictions_spiking, train_predictions_behavior = self.model(train_input)\n",
        "        behavior_output = train_output[:,:,-2:]\n",
        "        spiking_output = train_output[:,:,:-2]\n",
        "        # learn only from masked inputs, and learn only from the behavioral error in prediction\n",
        "        loss_behavior = torch.nn.MSELoss(train_predictions_behavior, behavior_output)\n",
        "        # learn only from masked inputs\n",
        "        loss_spiking = torch.nn.functional.poisson_nll_loss(train_predictions_spiking, spiking_output, log_input=False)\n",
        "        loss_behavior.backward()\n",
        "        self.optimizer.step()\n",
        "        # get validation score\n",
        "        train_res, train_output = self.score(train_input, train_output, prefix='train')\n",
        "        val_res, val_output = self.score(val_input, val_output, prefix='val')\n",
        "        res = train_res.copy()\n",
        "        res.update(val_res)\n",
        "        return res, (train_output, val_output)\n",
        "\n",
        "    def train(self, n_iter=1000, patience=200, save_path=None, verbose=False, log_frequency=50):\n",
        "        \"\"\"Trains model for given number of iterations with early stopping\"\"\"\n",
        "        train_log = []\n",
        "        best_score = 1e8\n",
        "        last_improv = -1\n",
        "        for i in range(n_iter):\n",
        "            res, output = self.train_epoch()\n",
        "            res['iter'] = i\n",
        "            train_log.append(res)\n",
        "            if verbose:\n",
        "                if (i % log_frequency) == 0:\n",
        "                    print(res)\n",
        "            if res['val_nll'] < best_score:\n",
        "                best_score = res['val_nll']\n",
        "                last_improv = i\n",
        "                data = res.copy()\n",
        "                if save_path is not None:\n",
        "                    self.save_checkpoint(save_path, data)\n",
        "            if (i - last_improv) > patience:\n",
        "                break\n",
        "        return train_log\n",
        "    \n",
        "    def save_checkpoint(self, file_path, data):\n",
        "        default_ckpt = {\n",
        "            \"state_dict\": self.model.state_dict(),\n",
        "            \"optim_state\": self.optimizer.state_dict(),\n",
        "        }\n",
        "        assert \"state_dict\" not in data\n",
        "        assert \"optim_state\" not in data\n",
        "        default_ckpt.update(data)\n",
        "        torch.save(default_ckpt, file_path)"
      ],
      "metadata": {
        "id": "jn71BJ5Ax_U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lxFWpuzORVf",
        "outputId": "af3cb662-6cef-45a7-9651-4f599a096d60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = hariRNN(input_dim=training_input.shape[2], hidden_dim=40, output_dim=training_output.shape[2])\n",
        "ckpt = torch.load('pretrained_rnn.ckpt')\n",
        "model.load_state_dict(ckpt['state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnSehxuaORVg"
      },
      "source": [
        "### 3.4 Model Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alX3uL8bORVg"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_inference.png?raw=true\" width=\"480\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uf3mS_VORVg"
      },
      "source": [
        "Finally, we'll generate our training and evaluation predictions by passing the data through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98V9WIdUORVg"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "training_predictions = model(training_input).cpu().detach().numpy()\n",
        "eval_predictions = model(eval_input).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qU90DfrORVg"
      },
      "source": [
        "## 4 Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6apJ3JOIORVh"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_submission.png?raw=true\" width=\"480\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90d8us5ORVh"
      },
      "source": [
        "### 4.1 File Preparation\n",
        "\n",
        "Now that we have predictions for the training and evaluation data, we can prepare a submission. The submission has a similar format to the returned data tensor dictionaries, but with an additional layer specifying the dataset.\n",
        "\n",
        "The dataset name and array names must be correct in order for the automated evaluation to work properly, as shown below. `'eval_rates_heldin_forward'` and `'eval_rates_heldout_forward'` are required only if you would like results on the optional forward prediction metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBuZOxmrORVh"
      },
      "outputs": [],
      "source": [
        "tlen = train_dict['train_spikes_heldin'].shape[1]\n",
        "num_heldin = train_dict['train_spikes_heldin'].shape[2]\n",
        "\n",
        "submission = {\n",
        "    'mc_maze_large': {\n",
        "        'train_rates_heldin': training_predictions[:, :tlen, :num_heldin],\n",
        "        'train_rates_heldout': training_predictions[:, :tlen, num_heldin:],\n",
        "        'eval_rates_heldin': eval_predictions[:, :tlen, :num_heldin],\n",
        "        'eval_rates_heldout': eval_predictions[:, :tlen, num_heldin:],\n",
        "        'eval_rates_heldin_forward': eval_predictions[:, tlen:, :num_heldin],\n",
        "        'eval_rates_heldout_forward': eval_predictions[:, tlen:, num_heldin:]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR0WWVYSORVi"
      },
      "source": [
        "These dicts must be in an HDF5 format to be submitted to EvalAI. We have a function called `save_to_h5` to save these dictionaries to HDF5 files while preserving the structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWOOaMbYORVi"
      },
      "outputs": [],
      "source": [
        "# from nlb_tools.make_tensors import save_to_h5\n",
        "\n",
        "# save_to_h5(submission, 'submission.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHkXJ5wJORVi"
      },
      "source": [
        "### 4.2 Submission Upload\n",
        "\n",
        "The files can be submitted through the EvalAI website or using their CLI tool. The CLI tool is recommended for large files (>300 MB), but there is no difference for smaller files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWajWLEsORVi"
      },
      "outputs": [],
      "source": [
        "# Configure EvalAI-CLI with your account credentials\n",
        "# !evalai set_token <auth_token>\n",
        "\n",
        "# Our challenge's id is 1256, and the phase ids are 2539 for Validation and 2540 for Test\n",
        "# So, to submit to the Validation phase of NLB'21:\n",
        "# !evalai challenge 1256 phase 2539 submit --file submission.h5\n",
        "\n",
        "# and if the file is large:\n",
        "# !evalai challenge 1256 phase 2539 submit --file submission.h5 --large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjOVqZNgORVj"
      },
      "source": [
        "See [this page](https://cli.eval.ai/) for more info on the EvalAI-CLI tool.\n",
        "\n",
        "Once your file is submitted, you can log in to EvalAI, go to our [challenge](https://eval.ai/web/challenges/challenge-page/1256/overview), and view the evaluation results in the 'My Submissions' tab. If your submission errored in evaluation, you can see the error output to assist in debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COOkjMLFORVj"
      },
      "source": [
        "## 5 Local Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eelnNS0lORVj"
      },
      "source": [
        "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_evaluation.png?raw=true\" width=\"600\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z0kU0GNORVk"
      },
      "source": [
        "\n",
        "For the Test Phase, submissions must be uploaded to EvalAI for evaluation. For the Validation Phase, submissions can also be evaluated locally with provided data and functions.\n",
        "\n",
        "First, we prepare the data used for evaluation with `make_eval_target_tensors`. This function extracts all necessary evaluation data from the loaded dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECOW75_cORVk"
      },
      "outputs": [],
      "source": [
        "from nlb_tools.make_tensors import make_eval_target_tensors\n",
        "\n",
        "target_dict = make_eval_target_tensors(dataset=dataset, \n",
        "                                       dataset_name='mc_maze_large',\n",
        "                                       train_trial_split='train',\n",
        "                                       eval_trial_split='val',\n",
        "                                       include_psth=True,\n",
        "                                       save_file=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE62nDbFORVk"
      },
      "source": [
        "Then, we can evaluate with the `evaluate` function. Every submission is scored on a number of metrics, each evaluating different aspects of the model output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sovwbqSTORVl",
        "outputId": "5f97e3d0-2ced-4451-fc5e-70146b753c8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'mc_maze_scaling_split': {'[500] co-bps': 0.3211858864626676,\n",
              "   '[500] vel R2': 0.856816843986107,\n",
              "   '[500] psth R2': 0.5743404432814287,\n",
              "   '[500] fp-bps': 0.18072337679652054}}]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nlb_tools.evaluation import evaluate\n",
        "\n",
        "evaluate(target_dict, submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6_xQi7hORVl"
      },
      "source": [
        "## 6 Conclusion\n",
        "\n",
        "This tutorial gave an overview of the NLB'21 pipeline from loading raw data to submission and demonstrated how to make use of our provided code package, `nlb_tools`, to participate in NLB'21.\n",
        "\n",
        "For additional helpful resources, we have a number of other tutorials and example scripts covering a variety of topics:\n",
        "* The notebooks in the [`nlb_tools` repo](https://github.com/neurallatents/nlb_tools) demonstrate application of classical methods like spike smoothing, GPFA, and SLDS to NLB'21.\n",
        "* Andrew Sedler's [nlb-lightning](https://github.com/arsedler9/nlb-lightning) package provides a convenient framework to develop and evaluate PyTorch Lightning models for NLB'21."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "02b1bc2cb6aba3c63676f81fd881bdec751fcef939c531164eae59d8a44feb6a"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Copy of nlb_technical_walkthrough.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}